---
title: "PEC 4 Machine Learning"
author: "Sheddad Kaid-Salah Ferrón"
date: "`r format(Sys.Date())`"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: yes
    toc_depth: 3
    theme: flatly
    number_sections: false
    csl: vancouver.csl
  pdf_document:
    toc: yes
params:
  date: !r Sys.Date()
  printcode:
    label: "Display Code:"
    value: TRUE # or set it to FALSE
  data:
    label: "Input dataset:"
    value: ECGCvdata.csv
    input: file
  folder.data: ""
  p.train: !r 2/3
  subtitulo: Predicción de dolencias cardiacas a partir de electrocardiograma
  seed: 12345
nocite: |
  @lantz2015machine
  @pec_ml_4
bibliography: PEC_4_ML.bib
geometry: margin=2cm
---

```{r class.source = 'fold-hide', setup, include=FALSE}
# Parametrizamos
require(knitr)
require(rmdformats)

## Global options
options(max.print="75")
opts_chunk$set(echo=params$printcode,
	             cache=TRUE, # para no reproducir cuando no hay cambios 
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
Sys.setlocale("LC_TIME", "C")
```

``` {r paquetes, include=FALSE}
# Instalamos paquetes, si no lo están, en caso de ser requeridos
if(!(require(knitr))) install.packages("knitr")
if(!(require(rmdformats))) install.packages("rmdformats")
if(!(require(reticulate))) install.packages("reticulate")
if(!(require(keras))) install.packages("keras")
if(!(require(dplyr))) install.packages("dplyr")
if(!(require(e1071))) install.packages("e1071")
if(!(require(kernlab))) install.packages("kernlab")
if(!(require(caret))) install.packages("caret")
if(!(require(ggplot2))) install.packages("ggplot2")
if(!(require(C50))) install.packages("C50")
if(!(require(randomForest))) install.packages("randomForest")
if(!(require(bibtex))) install.packages("bibtex")
```


```{r ficheros, include=FALSE}
# Fichero human_data.txt con los datos de las secuencias con la clase de proteínas 
# Lo hacemos parametrizado
data <- params$data
```

-----------------------------------------------------------

# 1. Introducción

Se dispone del resultado de 1200 electrocardiogramas (ECG) en pacientes con 
algún de estos 4 tipos de problemas cardíacos: Arrhythmia (ARR), Congestive 
Heart Failure (CHF), Atrial Fibrillation (AFF), Normal Sinus Rhythm (NSR). 
El número de variables recogidas es de 54. La primera variable RECORD se usa 
como identificador. La ultima variable corresponde a la clase y el resto son las
variables explicativas de tipo numérico. Como muestra se presentan las 8 
primeras:  

• hbpermin: Heart beat per minute
• Pseg: P wave segment length in ms
• PQseg: PQ segment length in ms
• QRSseg: QRS segment length in ms
• QRseg: QR segment length in ms
• QTseg: QT segment length in ms
• RSseg: RS segment length in ms
• STseg: ST segment length in ms
  
# 2. Objetivo
  
El objetivo de este análisis es **predecir el tipo de dolencia cardíaca** de los 
pacientes a partir de la información recogida en el ECG. Los datos se encuentran
en el fichero ECGCvdata.csv. La última columna contiene la clase.  

En esta PEC se analizan los datos mediante la **implementación** de los 
diferentes **algoritmos estudiados:**  
*k-Nearest Neighbour, Naive Bayes, Artificial Neural Network, Support Vector 
Machine, Arbol de Decisión y Random Forest* para **predecir** el tipo de 
docencia cardíaca.


# 3 Datos

## Lectura de datos

Leemos los datos del fichero *ECGCvdata.csv*. 

```{r chunck_3_1, echo = TRUE}
# Cargamos el archivo ECGCvdata.csv en un DataFrame
ecgcv_rawdata_df <- read.csv(data)

# Obtenemos el número de filas y columnas
dimensiones <- dim(ecgcv_rawdata_df)
num_filas <- dimensiones[1]
num_columnas <- dimensiones[2]
```

Nuestro fichero tiene `r num_filas` filas y `r num_columnas` columnas.

## Exploración de datos

El número de variables que tenemos en los datos es de `r num_columnas - 2`.   
Vemos si hay valores ausentes. En caso de haberlos, eliminaremos las variables 
(columnas) que los contengan.  

```{r chunck_3_2, echo = TRUE}
# Cargamos el paquete dplyr Lo utilizaremos para operar con los df
require(dplyr)

# Extraemos las dos primeras columnas de nuestro df y las guardamos.
id_class_columnas <- ecgcv_rawdata_df %>% select (RECORD, ECG_signal)
# Data Frame sin las columnas "RECORD" y "ECG_signal" (class)
ecgcv_rawdata_sin_class_df <- ecgcv_rawdata_df %>% select (-RECORD, -ECG_signal)
```
  
Eliminamos las variables con datos ausentes ("missing").  

```{r chunck_3_3, echo = TRUE}
# Verificamos que no haya valores anómalos (nulos) en el DataFrame

# Identificamos las columnas que contienen al menos un NaN
columnas_con_NaN <- apply(ecgcv_rawdata_sin_class_df, 2, function(x) any(is.nan(x)))
# Excluimos las columnas con NaN del dataframe
ecgcv_data_sin_NaN_df <- ecgcv_rawdata_sin_class_df[, !columnas_con_NaN]
num_columnas_sin_NaN <- dim(ecgcv_data_sin_NaN_df)[2]
# Añadimos la columna "ECG_signal" (es la "class")
ecgcv_data_df <- cbind(id_class_columnas[,2], ecgcv_data_sin_NaN_df)
# Añadimos nombres a la primera columna "class" (ECG_signal)
names(ecgcv_data_df)[1] <- c("class")
# Convertimos la columna 'class' en factor
ecgcv_data_df$class <- factor(ecgcv_data_df$class)
```

Finalmente reducimos el número de variables a `r num_columnas_sin_NaN`. 
  
## Gráfico de barras

Realizamos un boxplot de la variable clase.  

```{r chunck_3_4, echo = TRUE}
require(ggplot2)

ggplot(ecgcv_data_df, aes(x=class, fill=class)) +
  geom_bar() +
  theme_minimal() +
  labs(title='Distribución de las Clases')
```
  
Vemos que la distribución entre las cuatro clases es homogénea.  

## Gráfico de Correlación

Ahora observamos qué correlaciones puede haber en los datos, en concreto, entre 
las distintas variables.  

```{r chunck_3_5, echo = TRUE}
library(corrplot)
# Hacemos la matriz de correlación
corr_matrix <- cor(ecgcv_data_sin_NaN_df)
# Representamos la matriz de correlación
corrplot(corr_matrix, method = "color", title = "", type = "lower")
```

Lo ideal sería que no hubiera correlación para que los modelos sean robustos.
Este gráfico es interesante ya que nos podemos plantear a poteriori eliminar 
parte de los datos que estén muy correlacionados para simplificar los modelos.
    

## Separación de datos Training/Test
  
Utilizamos la semilla aleatoria 12345 para separar los datos en dos partes, una 
parte para training (67%) y una parte para test (33%).
  
  
```{r chunck_3_b_1}
# Separamos la base de datos en dos grupos: el grupo data.train con el 67 % de 
# las observaciones (params$p.train) y el grupo data.test con el resto

# Fijamos la semilla para que los cálculos pseudo-aleatorios sean reproducibles
set.seed(params$seed)

# Obtenemos el número total de filas (datos) de parkinson
n <- nrow(ecgcv_data_df)
# Calculamos el número de filas para entrenamiento y pruebas
n_train <- round(params$p.train * n) # 67%
n_test <- n - n_train # 33%
# Obtenemos los índices de filas al azar para entrenamiento
indices_train <- sample(1:n, n_train)
# Seleccionamos las filas para entrenamiento
data.train <- ecgcv_data_df[indices_train,]
# Seleccionamos las filas restantes para pruebas
data.test <- ecgcv_data_df[-indices_train, ]

require(dplyr)
# Seleccionamos los datos de "train" sin la clase
data.train_sin <- data.train %>% select(-class)
# Creamos los labels de class de "train"
train_labels <- data.train[, 1]
# Seleccionamos los datos de "test" sin la clase
data.test_sin <- data.test %>% select(-class)
# Creamos los labels de class de "test"
test_labels <- data.test[, 1]
```

# 4 Moldelos ML de clasificación

## a) k-Nearest Neighbour kNN {.tabset .tabset-fade .tabset-pills}

### k = 1

```{r chunck_4_a_1}

# Utilizamos la función knn del paquete "class"
require("class")

# Modelo knn utilizando una k = 1
mod_knn_1 <- knn(train = data.train_sin, test = data.test_sin, 
                  cl = train_labels, k = 1, prob = TRUE)

# Creamos un data frame con los resultados predichos y actuales
resultados_knn_1 <- data.frame(predichos = mod_knn_1, actuales = test_labels)

# Evaluamos el modelo
library(caret)
# Utilizamos la función confusionMatrix del paquete caret
conf.mat.knn_1 <- confusionMatrix(resultados_knn_1$predichos, resultados_knn_1$actuales)
conf.mat.knn_1

# Obtenemos la precisión (accuracy)
accuracy_knn_1 <- conf.mat.knn_1$overall["Accuracy"]
# Obtenemos el valor del coeficiente kappa
kappa_knn_1 <- conf.mat.knn_1$overall["Kappa"]
```

### k = 3

```{r chunck_4_a_2}

# Modelo knn utilizando una k = 3
mod_knn_3 <- knn(train = data.train_sin, test = data.test_sin, 
                  cl = train_labels, k = 3, prob = TRUE)
# Creamos un data frame con los resultados predichos y actuales
resultados_knn_3 <- data.frame(predichos = mod_knn_3, actuales = test_labels)
# Utilizamos la función confusionMatrix del paquete caret
conf.mat.knn_3 <- confusionMatrix(resultados_knn_3$predichos, resultados_knn_3$actuales)
conf.mat.knn_3
# Obtenemos la precisión (accuracy)
accuracy_knn_3 <- conf.mat.knn_3$overall["Accuracy"]
# Obtenemos el valor del coeficiente kappa
kappa_knn_3 <- conf.mat.knn_3$overall["Kappa"]
```

### k = 5

```{r chunck_4_a_3}

# Modelo knn utilizando una k = 5
mod_knn_5 <- knn(train = data.train_sin, test = data.test_sin, 
                  cl = train_labels, k = 5, prob = TRUE)
# Creamos un data frame con los resultados predichos y actuales
resultados_knn_5 <- data.frame(predichos = mod_knn_5, actuales = test_labels)
# Utilizamos la función confusionMatrix del paquete caret
conf.mat.knn_5 <- confusionMatrix(resultados_knn_5$predichos, resultados_knn_5$actuales)
conf.mat.knn_5
# Obtenemos la precisión (accuracy)
accuracy_knn_5 <- conf.mat.knn_5$overall["Accuracy"]
# Obtenemos el valor del coeficiente kappa
kappa_knn_5 <- conf.mat.knn_5$overall["Kappa"]
```

### k = 7

```{r chunck_4_a_4}

# Modelo knn utilizando una k = 7
mod_knn_7 <- knn(train = data.train_sin, test = data.test_sin, 
                  cl = train_labels, k = 7, prob = TRUE)
# Creamos un data frame con los resultados predichos y actuales
resultados_knn_7 <- data.frame(predichos = mod_knn_7, actuales = test_labels)
# Utilizamos la función confusionMatrix del paquete caret
conf.mat.knn_7 <- confusionMatrix(resultados_knn_7$predichos, resultados_knn_7$actuales)
conf.mat.knn_7
# Obtenemos la precisión (accuracy)
accuracy_knn_7 <- conf.mat.knn_7$overall["Accuracy"]
# Obtenemos el valor del coeficiente kappa
kappa_knn_7 <- conf.mat.knn_7$overall["Kappa"]
```

### Resultados knn

Modelo|Accuraccy|kappa|95% CI
:----|:----|:----:|:----:
k=1|`r accuracy_knn_1`|`r kappa_knn_1`|(`r conf.mat.knn_1$overall["AccuracyLower"]` - `r conf.mat.knn_1$overall["AccuracyUpper"]`)
k=3|`r accuracy_knn_3`|`r kappa_knn_3`|(`r conf.mat.knn_3$overall["AccuracyLower"]` - `r conf.mat.knn_3$overall["AccuracyUpper"]`)
k=5|`r accuracy_knn_5`|`r kappa_knn_5`|(`r conf.mat.knn_5$overall["AccuracyLower"]` - `r conf.mat.knn_5$overall["AccuracyUpper"]`)
k=7|`r accuracy_knn_7`|`r kappa_knn_7`|(`r conf.mat.knn_7$overall["AccuracyLower"]` - `r conf.mat.knn_7$overall["AccuracyUpper"]`)
  
Si nos fijamos en la información que nos da la función ConfusionMatrix() vemos 
que la k que da mejores resultados es la k=1. En este caso la «accuracy», que es
la probabilidad de que el modelo prediga correctamente un verdadero positivo o 
un verdadero negativo, es de `r accuracy_knn_1` mientras que en las otras k es de 
`r accuracy_knn_3`, `r accuracy_knn_5` y `r accuracy_knn_7`.
Si nos fijamos en el estadístico *Kappa*, que ajusta la «accuracy» teniendo en 
cuenta una correcta predicción por azar, vemos que también con k = 1 obtenemos 
el mejor resultado, `r round(kappa_knn_1, digits = 3)` que se considera 
«Good agreement» aunque que con las otras tres k’s también obtenemos un 
«Good agreement».


## b) Naive Bayes {.tabset .tabset-fade .tabset-pills}

### laplace = 0

```{r chunck_4_b_1}

require(e1071)
mod_naive.bayes <- naiveBayes(data.train_sin, train_labels, laplace=0)
#probabilidades condicionadas
#mod_naive.bayes$tables[1:4]

# Predicción del modelo Naive Bayes
pred_naive.bayes <- predict(mod_naive.bayes, data.test_sin)

# Métricas de rendimiento
#require(caret)
# Matriz de confusión
res <- table(pred_naive.bayes, test_labels)
conf.mat.naive.bayes <- confusionMatrix(res)
# Visualizamos
conf.mat.naive.bayes
```
  
### laplace = 1

```{r chunck_4_b_2}

require(e1071)
mod_naive.bayes_l1 <- naiveBayes(data.train_sin, train_labels, laplace=1)
#probabilidades condicionadas
#mod_naive.bayes$tables[1:4]

# Predicción del modelo Naive Bayes
pred_naive.bayes_l1 <- predict(mod_naive.bayes_l1, data.test_sin)

# Métricas de rendimiento
#require(caret)
# Matriz de confusión
res <- table(pred_naive.bayes_l1, test_labels)
conf.mat.naive.bayes_l1 <- confusionMatrix(res)
# Visualizamos
conf.mat.naive.bayes_l1
```
### Resultados Naive Bayes

Modelo|Accuraccy|kappa|95% CI
:----|:----|:----:|:----:
NaiveBayes l = 0|`r conf.mat.naive.bayes$overall["Accuracy"]`|`r conf.mat.naive.bayes$overall["Kappa"]`|(`r conf.mat.naive.bayes$overall["AccuracyLower"]` - `r conf.mat.naive.bayes$overall["AccuracyUpper"]`)
NaiveBayes l = 1|`r conf.mat.naive.bayes_l1$overall["Accuracy"]`|`r conf.mat.naive.bayes_l1$overall["Kappa"]`|(`r conf.mat.naive.bayes_l1$overall["AccuracyLower"]` - `r conf.mat.naive.bayes_l1$overall["AccuracyUpper"]`)

Los resultados con ambas condiciones, `laplace = 0` y `laplace = 1`, son 
idénticos. Nos quedamos con el modelo más simple así que no tiene sentido 
aplicar `laplace = 1` para mejorar el modelo.

## c) ANN {.tabset .tabset-fade .tabset-pills}

Lo primero que hacemos es normalizar los datos para poder realizar los modelos 
de Artificial Neural Networking (ANN). 

```{r chunck_4_c_0}
# Creamos la función para normalizar con la transformación minmax
normalize_min_max <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# Aplicar a una columna o a un dataframe completo
data.train_sin.norm <- as.data.frame(lapply(data.train_sin, normalize_min_max))
data.test_sin.norm <- as.data.frame(lapply(data.test_sin, normalize_min_max))

# Convertimos los dataframes a matrix para poder ejecutar los modelos ANN
x_train <- as.matrix(data.train_sin.norm)
x_test <- as.matrix(data.test_sin.norm)
# Restamos 1 ya que las categorías comienzan en 1
y_train <- as.integer(train_labels) - 1 
y_test <- as.integer(test_labels) - 1 
```

### 15 nodos en una capa oculta

Definimos el modelo y mostramos un resumen del mismo para ver las capas.

```{r chunck_4_c_1, eval = TRUE}
# Para ejecutar "keras" usamos el entorno virtual en el que està instalado "tensorflow"
require(reticulate)
use_python("/home/sheddad/anaconda3/envs/venv_UOC_ML_conda/bin/python3.10", 
           required = TRUE)

# Cargamos la librería keras que contiene tensorflow
require(keras)

# Definimos el modelo
mod_ANN_1 <- keras_model_sequential() 
# Añadimos una capa oculta densa de 15 nodos con activación relu
input_shape <- c(dim(data.train_sin)[2])
mod_ANN_1 %>% 
  layer_dense(units = 15, input_shape = c(dim(data.train_sin)[2]), activation = 'relu') %>%
  # Añadimos un Dropout del 20%
  layer_dropout(rate = 0.2) %>%
  # Generamos la capa de salida con activación softmax (4 nodos para las 4 clases)
  layer_dense(units = 4, activation = 'softmax')

# Mostramos el resumen del modelo
summary(mod_ANN_1)
```
Compilamos y ejecutamos el modelo.  


```{r chunck_4_c_2}
# Compilamos el modelo
mod_ANN_1 %>% compile(
  loss = 'sparse_categorical_crossentropy',
  optimizer = 'adam',
  metrics = c('accuracy')
)

# Hacemos que pare en caso de que ya no se mejore más
early_stopping <- callback_early_stopping(
  monitor = 'val_loss',
  patience = 10,
  restore_best_weights = TRUE)

# Ejecutamos el modelo 
# Épocas
epochs = 50
historia_mod_ANN_1 <- mod_ANN_1 %>% fit(
  x_train, y_train,
  epochs = epochs,
  batch_size = 32,
  validation_split = 0.2, 
  callbacks=early_stopping)
```

```{r chunck_4_c_3, eval = TRUE}
library(ggplot2)

accuracy <- historia_mod_ANN_1$metrics['accuracy']
val_accuracy <- historia_mod_ANN_1$metrics['val_accuracy']
epochs <- 1:epochs 

# Creamos un dataframe a partir de los valores
historia_df <- data.frame(epoch = epochs, accuracy = accuracy, 
                          val_accuracy = val_accuracy)
# Representamos ambas curvas
ggplot(historia_df, aes(x=epoch)) +
  geom_line(aes(y=accuracy, colour="Entrenamiento")) +
  geom_line(aes(y=val_accuracy, colour="Validación")) +
  labs(title='Curva de Aprendizaje del Modelo 1',
       x='Épocas',
       y='Accuracy',
       colour="Leyenda") +
  theme_minimal()
```

Predicción sobre los valores del test.

```{r chunck_4_c_4, eval = TRUE}
# Predicción con "keras" del test con el modelo ANN que produce probabilidades
prob_mod.ANN_1 <- mod_ANN_1 %>% predict(x_test, axis=1)
# Convertimos las probabilidades en clases 
pred_mod.ANN_1 <- apply(prob_mod.ANN_1, 1, which.max)
#y las hacemos factores
pred_mod.ANN_1 <- factor(pred_mod.ANN_1, labels = c("AFF", "ARR", "CHF", "NSR"))

# Métricas de rendimiento con "caret"
# Matriz de confusión
res <- table(pred_mod.ANN_1, test_labels)
conf.mod.ANN_1 <- confusionMatrix(res)
# Visualizamos
conf.mod.ANN_1
```

### 25 y 10 nodos en dos capas ocultas

Definimos el modelo y mostramos un resumen del mismo para ver las capas.

```{r chunck_4_c_5}
# Para ejecutar "keras" usamos el entorno virtual en el que està instalado "tensorflow"
require(reticulate)
use_python("/home/sheddad/anaconda3/envs/venv_UOC_ML_conda/bin/python3.10", 
           required = TRUE)

# Cargamos la librería keras que contiene tensorflow
require(keras)

# Definimos el modelo
mod_ANN_2 <- keras_model_sequential() 
# Añadimos una capa oculta densa de 15 nodos con activación relu
input_shape <- c(dim(data.train_sin)[2])
mod_ANN_2 %>% 
  # Primera capa de 25 nodos
  layer_dense(units = 25, input_shape = c(dim(data.train_sin)[2]), 
              activation = 'relu') %>%
  # Añadimos un Dropout del 20%
  layer_dropout(rate = 0.2) %>%
  # Segunda capa de 10 nodos
  layer_dense(units = 10, input_shape = c(dim(data.train_sin)[2]), 
              activation = 'relu') %>%
  # Añadimos un Dropout del 20%
  layer_dropout(rate = 0.2) %>%
  # Generamos la capa de salida con activación softmax (4 nodos para las 4 clases)
  layer_dense(units = 4, activation = 'softmax')

# Mostramos el resumen del modelo
summary(mod_ANN_2)
```

Compilamos y ejecutamos el modelo.  


```{r chunck_4_c_6}
# Compilamos el modelo
mod_ANN_2 %>% compile(
  loss = 'sparse_categorical_crossentropy',
  optimizer = 'adam',
  metrics = c('accuracy')
)

# Hacemos que pare en caso de que ya no se mejore más
early_stopping <- callback_early_stopping(
  monitor = 'val_loss',
  patience = 10,
  restore_best_weights = TRUE)

# Ejecutamos el modelo 
# Épocas
epochs = 50
historia_mod_ANN_2 <- mod_ANN_2 %>% fit(
  x_train, y_train,
  epochs = epochs,
  batch_size = 32,
  validation_split = 0.2, 
  callbacks=early_stopping)
```

```{r chunck_4_c_7}
library(ggplot2)

accuracy <- historia_mod_ANN_2$metrics['accuracy']
val_accuracy <- historia_mod_ANN_2$metrics['val_accuracy']
epochs <- 1:epochs 

# Creamos un dataframe a partir de los valores
historia_df <- data.frame(epoch = epochs, accuracy = accuracy, 
                          val_accuracy = val_accuracy)
# Representamos ambas curvas
ggplot(historia_df, aes(x=epoch)) +
  geom_line(aes(y=accuracy, colour="Entrenamiento")) +
  geom_line(aes(y=val_accuracy, colour="Validación")) +
  labs(title='Curva de Aprendizaje del Modelo 2',
       x='Épocas',
       y='Accuracy',
       colour="Leyenda") +
  theme_minimal()
```

Predicción sobre los valores del test. 

```{r chunck_4_c_8}
# Predicción con "keras" del test con el modelo ANN que produce probabilidades
prob_mod.ANN_2 <- mod_ANN_2 %>% predict(x_test, axis=1)
# Convertimos las probabilidades en clases 
pred_mod.ANN_2 <- apply(prob_mod.ANN_2, 1, which.max)
#y las hacemos factores
pred_mod.ANN_2 <- factor(pred_mod.ANN_2, labels = c("AFF", "ARR", "CHF", "NSR"))

# Métricas de rendimiento con "caret"
# Matriz de confusión
res <- table(pred_mod.ANN_2, test_labels)
conf.mod.ANN_2 <- confusionMatrix(res)
# Visualizamos
conf.mod.ANN_2
```

### Resultados ANN

Modelo|Accuraccy|kappa|95% CI
:----|:----|:----:|:----:
ANN 1 capa 15 nodos|`r conf.mod.ANN_1$overall["Accuracy"]`|`r conf.mod.ANN_1$overall["Kappa"]`|(`r conf.mod.ANN_1$overall["AccuracyLower"]` - `r conf.mod.ANN_1$overall["AccuracyUpper"]`)
ANN 2 capas 25 y 10 nodos|`r conf.mod.ANN_2$overall["Accuracy"]`|`r conf.mod.ANN_2$overall["Kappa"]`|(`r conf.mod.ANN_2$overall["AccuracyLower"]` - `r conf.mod.ANN_2$overall["AccuracyUpper"]`)


Vemos que el modelo con una capa y 15 nodos es el que da mejores resutados.

## d) Modelos SVM {.tabset .tabset-fade .tabset-pills}
  
Utilizamos el kernel lineal y el kernel RBF para crear sendos modelos SVM 
basados en el training para predecir las clases en los datos del test.

### kernel lineal

Relalizamos el modelo predictivo con el kernel lineal.
  
```{r chunck_4_d_1}
require(kernlab)
# Hacemos el clasificador con el modelo lineal ksvm. 
mod_ksvm_lineal <- ksvm(class ~ ., data = data.train, kernel = "vanilladot")
# Vemos la información del modelo
mod_ksvm_lineal
```

Hacemos la predicción de los datos del test con el kernel lineal  

```{r chunck_4_d_2}
# Predicción del test con el kernel lineal
pred_ksvm_lineal <- predict(mod_ksvm_lineal, data.test)

# Métricas de rendimiento
require(caret)
# Matriz de confusión
res <- table(pred_ksvm_lineal, data.test$class)
conf.mat.ksvm.lineal <- confusionMatrix(res, positive="+")
# Visualizamos
conf.mat.ksvm.lineal
```

### kernel RBF

Relalizamos el modelo predictivo con el kernel RBF

```{r chunck_4_d_3}

#require(kernlab)
# Hacemos el clasificador con el modelo ksvm RBF.
mod_ksvm_RBF <- ksvm(class ~ ., data = data.train, kernel = "rbfdot")
# Vemos la información del modelo
mod_ksvm_RBF
```

Hacemos la predicción de los datos del test con el kernel RBF  

```{r chunck_4_d_4}
# Predicción del test con el kernel RBF
pred_ksvm_RBF <- predict(mod_ksvm_RBF, data.test)

# Métricas de rendimiento
# require(caret)
# Matriz de confusión
res <- table(pred_ksvm_RBF, data.test$class)
conf.mat.ksvm.RBF <- confusionMatrix(res, positive="+")
# Visualizamos
conf.mat.ksvm.RBF
```

### Resultados SVM

Modelo|Accuraccy|kappa|95% CI
:----|:----|:----:|:----:
ksvm.lineal|`r conf.mat.ksvm.lineal$overall["Accuracy"]`|`r conf.mat.ksvm.lineal$overall["Kappa"]`|(`r conf.mat.ksvm.lineal$overall["AccuracyLower"]` - `r conf.mat.ksvm.lineal$overall["AccuracyUpper"]`)
ksvm.RBF|`r conf.mat.ksvm.RBF$overall["Accuracy"]`|`r conf.mat.ksvm.RBF$overall["Kappa"]`|(`r conf.mat.ksvm.RBF$overall["AccuracyLower"]` - `r conf.mat.ksvm.RBF$overall["AccuracyUpper"]`)
  
Los modelos SVM entrenados con los datos que hemos separado para `ksvm` nos dan 
ligeramente distintos resultados. En igualdad de condiciones lo apropiado sería
escoger el modelo más simple. En todo caso, escogemos ksvm lineal que nos da 
mejores resultados y además es el modelo más simple.

## e) Árbol de Clasificación {.tabset .tabset-fade .tabset-pills} 

Los modelos de árbol de clasifición son capaces de tomar decisiones complejas a 
partir de conjuntos de elecciones simples. Esto los ha hecho populares debido a 
su simplicidad, facilidad de interpretación, dado que presentan su conocimiento 
en forma de estructuras lógicas que pueden entenderse sin conocimientos 
estadísticos y eficacia para una variedad de tareas de predicción.

### Sin boosting 

```{r chunck_4_e_1}
require(C50)
# Hacemos el clasificador con el modelo tree model. 
mod_arbol <- C5.0(data.train, train_labels, trials = 1, cost = NULL)
# Vemos la información del modelo
mod_arbol
# Vemos las tree's decistions
summary(mod_arbol)
```
  
Hacemos la predicción de los datos del test con nuestro modelo de árbol. 

```{r chunck_4_e_2}
# Predicción del tree model
pred_mod_arbol <- predict(mod_arbol, data.test)

# Métricas de rendimiento
# require(caret)
# Matriz de confusión
res <- table(pred_mod_arbol, test_labels)
conf.mat.mod_arbol <- confusionMatrix(res)
# Visualizamos
conf.mat.mod_arbol
```

### boosting activado

```{r chunck_4_e_3}
# Hacemos el clasificador con el modelo lineal ksvm. 
mod_arbol_boost10 <- C5.0(data.train, train_labels, trials = 10, cost = NULL)
# Vemos la información del modelo
mod_arbol_boost10
# Vemos las tree's decistions
summary(mod_arbol_boost10)
```

Hacemos la predicción de los datos del test con nuestro modelo de árbol con 
boosting activado. 

```{r chunck_4_e_4}
# Predicción del tree model
pred_mod_arbol_boost10 <- predict(mod_arbol_boost10, data.test)

# Métricas de rendimiento
# require(caret)
# Matriz de confusión
res <- table(pred_mod_arbol_boost10, test_labels)
conf.mat.mod_arbol_boost10 <- confusionMatrix(res)
# Visualizamos
conf.mat.mod_arbol_boost10
```

### Resultados modelos árbol de clasificación

Modelo|Accuraccy|kappa|95% CI
:----|:----|:----:|:----:
boostin no activado|`r conf.mat.mod_arbol$overall["Accuracy"]`|`r conf.mat.mod_arbol$overall["Kappa"]`|(`r conf.mat.mod_arbol$overall["AccuracyLower"]` - `r conf.mat.mod_arbol$overall["AccuracyUpper"]`)
boostin activado|`r conf.mat.mod_arbol_boost10$overall["Accuracy"]`|`r conf.mat.mod_arbol_boost10$overall["Kappa"]`|(`r conf.mat.mod_arbol_boost10$overall["AccuracyLower"]` - `r conf.mat.mod_arbol_boost10$overall["AccuracyUpper"]`)
  
Con ambos modelos, `boosting no activado` y `boosting activado`, obtenemos los 
mismos resultados. Dado que crear el modelo con 10 árboles es más costoso 
computacionalmente que hacerlo con un árbol, nos quedamos de nuevo con el modelo
más simple, con el modelo de árbol sin boosting activado.
  
Cabe comentar que con estos modelos conseguimos unas predicciones perfectas. 
Algo curioso que hace sospechar de 'overfiting'.

  
## f) Random Forest {.tabset .tabset-fade .tabset-pills}
  
Un Random Forest es esencialmente una colección de árboles de decisión, se 
centra únicamente en conjuntos de árboles de decisión donde cada árbol es 
ligeramente diferente de los demás.  
Se construyen mediante un método de muestreo con reemplazo (bootstrap sampling).
Cada árbol en el bosque se entrena en una muestra de datos ligeramente 
diferente.  
Algunas de sus fortalezas son que se pueden utilizar en múltiples problemas, 
pueden usar datos con ruido y manejar características tanto categóricas como 
contínuas y pueden suar datos con un elevado número de características.  
A diferencia de los árboles de de decisión, no son modelos fácilmente 
interpretables aunque tienden a no tener problemas de "overfiting".  
  

### n = 100 árboles

```{r chunck_4_f_1}
require(randomForest)
# Hacemos el clasificador con el modelo Random Forest. 
mod_rf_100 <- randomForest(data.train, train_labels, ntree = 100)
# Vemos la información del modelo
mod_rf_100
# Vemos las tree's decistions
summary(mod_rf_100)
```

```{r chunck_4_f_2}
# Predicción del tree model
pred_mod_rf_100 <- predict(mod_rf_100, data.test, type = "response")

# Métricas de rendimiento
# require(caret)
# Matriz de confusión
res <- table(pred_mod_rf_100, test_labels)
conf.mat.mod_rf_100 <- confusionMatrix(res)
# Visualizamos
conf.mat.mod_rf_100
```

### n = 200 árboles

```{r chunck_4_f_3}
require(randomForest)
# Hacemos el clasificador con el modelo Random Forest. 
mod_rf_200 <- randomForest(data.train, train_labels, ntree = 200)
# Vemos la información del modelo
mod_rf_200
# Vemos las tree's decistions
summary(mod_rf_200)
```

```{r chunck_4_f_4}
# Predicción del tree model
pred_mod_rf_200 <- predict(mod_rf_200, data.test, type = "response")

# Métricas de rendimiento
# require(caret)
# Matriz de confusión
res <- table(pred_mod_rf_200, test_labels)
conf.mat.mod_rf_200 <- confusionMatrix(res)
# Visualizamos
conf.mat.mod_rf_200
```

### Resultados modelos Random Forest

Modelo|Accuraccy|kappa|95% CI
:----|:----|:----:|:----:
100 árboles|`r conf.mat.mod_rf_100$overall["Accuracy"]`|`r conf.mat.mod_rf_100$overall["Kappa"]`|(`r conf.mat.mod_rf_100$overall["AccuracyLower"]` - `r conf.mat.mod_rf_100$overall["AccuracyUpper"]`)
200 árboles|`r conf.mat.mod_rf_200$overall["Accuracy"]`|`r conf.mat.mod_rf_200$overall["Kappa"]`|(`r conf.mat.mod_rf_200$overall["AccuracyLower"]` - `r conf.mat.mod_arbol$conf.mat.mod_rf_200["AccuracyUpper"]`)
  
De la misma manera que com los modelos de árboles, tanto con el modelo 
random forest con `100 árboles` como con el de `200 árboles` obtenemos los 
mismos resultados. Dado que crear el modelo con 200 árboles es más costoso 
computacionalmente que hacerlo con 200 árbololse, nos quedamos de nuevo con el 
modelo más simple, el de 100 árboles.
  
Comentar también que con estos modelos hemos conseguido unas predicciones 
perfectas. 

# Resultados y Discusión

Hemos utilizado distintos modelos de Machine Learnig (ML) para hacer modelos de 
predicción con unos datos de 1200 electrocardiogramas (ECG) en pacientes con 
4 tipos distintos de problemas cardíacos: ARR, CHF, AFF, NSR.  
El número de variables recogidas es de `r num_columnas` pero una vez filtrado 
los datos y habiendo eliminado las columnas que tenían datos ausentes (NaN) 
nos han quedado `r num_columnas_sin_NaN` variables.  

Hemos realizado el modelo k-Nearest Neighbour kNN con distitas k, dando el mejor
resultado con una k = 1.  
El modelo Naive Bayes ha dado buenos resultados. Lo hemos probado con el 
parámetro l=0 y l=1; nos han dado los mismos resultados por lo que nos hemos 
quedado con l = 0.
Los modelos ANN no han dado unos resultados tan buenos en comparación con los
otros modelos de ML. En todo caso, el modelo que mejor ha funcionado ha sido el
de una capa.  
Se han realizado dos modelos SVM, con el kernel lienal y con el kernel RBF 
obteniendo mejores resultados con el kernel lineal.
El modelo de Árbol de Clasificación lo hemos realizado sin activar y activando 
el *boosting*. Dada la igualdad de resultados nos hemos quedado con el modelo 
más simple, el de sin activación.  
Por último hemos realizado un modelo Random Forest con 100 y 200 árboles. De la 
misma, los resultados han sido idénticos y hemos elegido el modelo más simple, 
el de 100 árboles.

Veamos una la tabla comparativa con los modelos con mejores resultados

Modelo|Accuraccy|kappa|95% CI
:----|:----|:----:|:----:
kNN (k=1)|`r accuracy_knn_1`|`r kappa_knn_1`|(`r conf.mat.knn_1$overall["AccuracyLower"]` - `r conf.mat.knn_1$overall["AccuracyUpper"]`)
NaiveBayes (l=0)|`r conf.mat.naive.bayes$overall["Accuracy"]`|`r conf.mat.naive.bayes$overall["Kappa"]`|(`r conf.mat.naive.bayes$overall["AccuracyLower"]` - `r conf.mat.naive.bayes$overall["AccuracyUpper"]`)
ANN 1 capa 15 nodos|`r conf.mod.ANN_1$overall["Accuracy"]`|`r conf.mod.ANN_1$overall["Kappa"]`|(`r conf.mod.ANN_1$overall["AccuracyLower"]` - `r conf.mod.ANN_1$overall["AccuracyUpper"]`)
ksvm.lineal|`r conf.mat.ksvm.lineal$overall["Accuracy"]`|`r conf.mat.ksvm.lineal$overall["Kappa"]`|(`r conf.mat.ksvm.lineal$overall["AccuracyLower"]` - `r conf.mat.ksvm.lineal$overall["AccuracyUpper"]`)
Arbol de Calsificación (no)|`r conf.mat.mod_arbol$overall["Accuracy"]`|`r conf.mat.mod_arbol$overall["Kappa"]`|(`r conf.mat.mod_arbol$overall["AccuracyLower"]` - `r conf.mat.mod_arbol$overall["AccuracyUpper"]`)
Random Forest (100)|`r conf.mat.mod_rf_100$overall["Accuracy"]`|`r conf.mat.mod_rf_100$overall["Kappa"]`|(`r conf.mat.mod_rf_100$overall["AccuracyLower"]` - `r conf.mat.mod_rf_100$overall["AccuracyUpper"]`)

Si nos fijamos en la tabla, todos los modelos han dado muy buenos resultados en 
la clasificación y en la predicción. Los modelos de Árbol de Clasificación y 
Random Forest nos han dado una predicción perfecta. No deja de planear la idea 
de que quizá hayan tenido problemas de "overfiting". En todo caso, ellos serían 
los escogidos.

# Conclusiones

En este práctica hemos podido aplicar a una misma fuente de datos los distintos 
tipos de modelos que se han visto a lo largo del curso. Los resultados de los 
mismos han sido variables y se ha podido comprobar las distintas mejoras que 
acepta cada modelo observando que en la mayoría de los casos, por no decir todos,
el modelo más simple ha sido el que mejor rendimiento ha dado.  
Ha sido un trabajo bastante didáctico dada la visión general que se ha obtenido
en este trabajo final de la asignatura.  


# Referencias  

