# Métricas de rendimiento
#require(caret)
# Matriz de confusión
res <- table(pred_mod.ANN_1, y_test)
conf.mod.ANN_1 <- confusionMatrix(res)
# Métricas de rendimiento
#require(caret)
# Matriz de confusión
res <- table(pred_mod.ANN_1, test_labels)
conf.mod.ANN_1 <- confusionMatrix(res)
# Predicción del test con el modelo ANN que produce probabilidades
prob_mod.ANN_1 <- predict(modelo1, x_test, type = "prob")
View(prob_mod.ANN_1)
prob_mod.ANN_1 <- modelo1 %>% predict(x_test)
# Convertimos las probabilidades en clases y las hacemos factores
pred_mod.ANN_1 <- apply(prob_mod.ANN_1, 1, which.max)
# Métricas de rendimiento
#require(caret)
# Matriz de confusión
res <- table(pred_mod.ANN_1, test_labels)
conf.mod.ANN_1 <- confusionMatrix(res)
# Métricas de rendimiento
#require(caret)
# Matriz de confusión
res <- table(pred_mod.ANN_1, y_test)
conf.mod.ANN_1 <- confusionMatrix(res)
prob_mod.ANN_1 <- modelo1 %>% predict(x_train)
# Convertimos las probabilidades en clases y las hacemos factores
pred_mod.ANN_1 <- apply(prob_mod.ANN_1, 1, which.max)
# Métricas de rendimiento
#require(caret)
# Matriz de confusión
res <- table(pred_mod.ANN_1, y_train)
conf.mod.ANN_1 <- confusionMatrix(res)
prob_mod.ANN_1 <- modelo1 %>% predict(x_test,axis=1)
# Convertimos las probabilidades en clases y las hacemos factores
pred_mod.ANN_1 <- apply(prob_mod.ANN_1, 1, which.max)
# Métricas de rendimiento
#require(caret)
# Matriz de confusión
res <- table(pred_mod.ANN_1, test_labels)
conf.mod.ANN_1 <- confusionMatrix(res)
View(prob_mod.ANN_1)
# Parametrizamos
require(knitr)
require(rmdformats)
## Global options
options(max.print="75")
opts_chunk$set(echo=params$printcode,
cache=TRUE, # para no reproducir cuando no hay cambios
prompt=FALSE,
tidy=TRUE,
comment=NA,
message=FALSE,
warning=FALSE)
opts_knit$set(width=75)
Sys.setlocale("LC_TIME", "C")
# Instalamos paquetes, si no lo están, en caso de ser requeridos
if(!(require(knitr))) install.packages("knitr")
if(!(require(rmdformats))) install.packages("rmdformats")
if(!(require(reticulate))) install.packages("reticulate")
if(!(require(keras))) install.packages("keras")
if(!(require(dplyr))) install.packages("dplyr")
if(!(require(e1071))) install.packages("e1071")
if(!(require(kernlab))) install.packages("kernlab")
if(!(require(caret))) install.packages("caret")
if(!(require(ggplot2))) install.packages("ggplot2")
if(!(require(C50))) install.packages("C50")
if(!(require(randomForest))) install.packages("randomForest")
if(!(require(bibtex))) install.packages("bibtex")
# Fichero human_data.txt con los datos de las secuencias con la clase de proteínas
# Lo hacemos parametrizado
data <- params$data
# Cargamos el archivo ECGCvdata.csv en un DataFrame
ecgcv_rawdata_df <- read.csv(data)
# Obtenemos el número de filas y columnas
dimensiones <- dim(ecgcv_rawdata_df)
num_filas <- dimensiones[1]
num_columnas <- dimensiones[2]
# Cargamos el paquete dplyr Lo utilizaremos para operar con los df
require(dplyr)
# Extraemos las dos primeras columnas de nuestro df y las guardamos.
id_class_columnas <- ecgcv_rawdata_df %>% select (RECORD, ECG_signal)
# Data Frame sin las columnas "RECORD" y "ECG_signal" (class)
ecgcv_rawdata_sin_class_df <- ecgcv_rawdata_df %>% select (-RECORD, -ECG_signal)
# Verificamos que no haya valores anómalos (nulos) en el DataFrame
# Identificamos las columnas que contienen al menos un NaN
columnas_con_NaN <- apply(ecgcv_rawdata_sin_class_df, 2, function(x) any(is.nan(x)))
# Excluimos las columnas con NaN del dataframe
ecgcv_data_sin_NaN_df <- ecgcv_rawdata_sin_class_df[, !columnas_con_NaN]
num_columnas_sin_NaN <- dim(ecgcv_data_sin_NaN_df)[2]
# Añadimos la columna "ECG_signal" (es la "class")
ecgcv_data_df <- cbind(id_class_columnas[,2], ecgcv_data_sin_NaN_df)
# Añadimos nombres a la primera columna "class" (ECG_signal)
names(ecgcv_data_df)[1] <- c("class")
# Convertimos la columna 'class' en factor
ecgcv_data_df$class <- factor(ecgcv_data_df$class)
# Separamos la base de datos en dos grupos: el grupo data.train con el 67 % de
# las observaciones (params$p.train) y el grupo data.test con el resto
# Fijamos la semilla para que los cálculos pseudo-aleatorios sean reproducibles
set.seed(params$seed)
# Obtenemos el número total de filas (datos) de parkinson
n <- nrow(ecgcv_data_df)
# Calculamos el número de filas para entrenamiento y pruebas
n_train <- round(params$p.train * n) # 67%
n_test <- n - n_train # 33%
# Obtenemos los índices de filas al azar para entrenamiento
indices_train <- sample(1:n, n_train)
# Seleccionamos las filas para entrenamiento
data.train <- ecgcv_data_df[indices_train,]
# Seleccionamos las filas restantes para pruebas
data.test <- ecgcv_data_df[-indices_train, ]
require(dplyr)
# Seleccionamos los datos de "train" sin la clase
data.train_sin <- data.train %>% select(-class)
# Creamos los labels de class de "train"
train_labels <- data.train[, 1]
# Seleccionamos los datos de "test" sin la clase
data.test_sin <- data.test %>% select(-class)
# Creamos los labels de class de "test"
test_labels <- data.test[, 1]
# Creamos la función para normalizar con la transformación minmax
normalize_min_max <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
# Aplicar a una columna o a un dataframe completo
data.train_sin.norm <- as.data.frame(lapply(data.train_sin, normalize_min_max))
data.test_sin.norm <- as.data.frame(lapply(data.test_sin, normalize_min_max))
# Convertimos los dataframes a matrix para poder ejecutar los modelos ANN
x_train <- as.matrix(data.train_sin.norm)
x_test <- as.matrix(data.test_sin.norm)
# Restamos 1 ya que las categorías comienzan en 1
y_train <- as.integer(train_labels) - 1
y_test <- as.integer(test_labels) - 1
require(keras)
# Definimos el modelo
modelo1 <- keras_model_sequential()
# Añadimos una capa oculta densa de 15 nodos con activación relu
input_shape <- c(dim(data.train_sin)[2])
modelo1 %>%
layer_dense(units = 15, input_shape = c(dim(data.train_sin)[2]), activation = 'relu') %>%
# Añadimos un Dropout del 20%
layer_dropout(rate = 0.2) %>%
# Generamos la capa de salida con activación softmax (4 nodos para las 4 clases)
layer_dense(units = 4, activation = 'softmax')
# Mostramos el resumen del modelo
summary(modelo1)
modelo1 %>% compile(
loss = 'sparse_categorical_crossentropy',
optimizer = 'adam',
metrics = c('accuracy')
)
early_stopping <- callback_early_stopping(
monitor = 'val_loss',
patience = 10,
restore_best_weights = TRUE)
historia_modelo1 <- modelo1 %>% fit(
x_train, y_train,
epochs = 50,
batch_size = 32,
validation_split = 0.2)
require(keras)
# Definimos el modelo
modelo1 <- keras_model_sequential()
# Añadimos una capa oculta densa de 15 nodos con activación relu
input_shape <- c(dim(data.train_sin)[2])
modelo1 %>%
layer_dense(units = 15, input_shape = c(dim(data.train_sin)[2]), activation = 'relu') %>%
# Añadimos un Dropout del 20%
layer_dropout(rate = 0.2) %>%
# Generamos la capa de salida con activación softmax (4 nodos para las 4 clases)
layer_dense(units = 4, activation = 'softmax')
# Mostramos el resumen del modelo
summary(modelo1)
# Compilamos el modelo
modelo1 %>% compile(
loss = 'sparse_categorical_crossentropy',
optimizer = 'adam',
metrics = c('accuracy')
)
# Hacemos que pare en caso de que ya no se mejore más
early_stopping <- callback_early_stopping(
monitor = 'val_loss',
patience = 10,
restore_best_weights = TRUE)
# Ejecutamos el modelo
historia_modelo1 <- modelo1 %>% fit(
x_train, y_train,
epochs = 50,
batch_size = 32,
validation_split = 0.2,
callbacks=early_stopping)
prob_mod.ANN_1 <- modelo1 %>% predict(x_test, axis=1)
# Convertimos las probabilidades en clases y las hacemos factores
pred_mod.ANN_1 <- apply(prob_mod.ANN_1, 1, which.max)
# Métricas de rendimiento
#require(caret)
# Matriz de confusión
res <- table(pred_mod.ANN_1, test_labels)
conf.mod.ANN_1 <- confusionMatrix(res)
View(data.test)
pred_mod.ANN_1 <- factor(pred_mod.ANN_1, labels = c("AFF", "ARR", "CHF", "NSR"))
# Métricas de rendimiento
#require(caret)
# Matriz de confusión
res <- table(pred_mod.ANN_1, test_labels)
conf.mod.ANN_1 <- confusionMatrix(res)
# Visualizamos
conf.mat.ksvm.lineal
# Visualizamos
conf.mod.ANN_1
# Parametrizamos
require(knitr)
require(rmdformats)
## Global options
options(max.print="75")
opts_chunk$set(echo=params$printcode,
cache=TRUE, # para no reproducir cuando no hay cambios
prompt=FALSE,
tidy=TRUE,
comment=NA,
message=FALSE,
warning=FALSE)
opts_knit$set(width=75)
Sys.setlocale("LC_TIME", "C")
require(reticulate)
use_python("/home/sheddad/anaconda3/envs/venv_UOC_ML_conda/bin/python3.10", required = TRUE)
# Cargamos la librería keras que contiene tensorflow
require(keras)
# Definimos el modelo
modelo1 <- keras_model_sequential()
# Añadimos una capa oculta densa de 15 nodos con activación relu
input_shape <- c(dim(data.train_sin)[2])
modelo1 %>%
layer_dense(units = 15, input_shape = c(dim(data.train_sin)[2]), activation = 'relu') %>%
# Añadimos un Dropout del 20%
layer_dropout(rate = 0.2) %>%
# Generamos la capa de salida con activación softmax (4 nodos para las 4 clases)
layer_dense(units = 4, activation = 'softmax')
# Mostramos el resumen del modelo
summary(modelo1)
library(ggplot2)
View(historia_modelo1)
library(ggplot2)
# Suponiendo que tienes un dataframe 'historia' con las columnas 'epoch', 'accuracy' y 'val_accuracy'
# Por ejemplo: historia <- data.frame(epoch=1:n, accuracy=..., val_accuracy=...)
ggplot(historia_modelo, aes(x=epoch)) +
geom_line(aes(y=accuracy, colour="Entrenamiento")) +
geom_line(aes(y=val_accuracy, colour="Validación")) +
labs(title='Curva de Aprendizaje del Modelo 1',
x='Épocas',
y='Accuracy',
colour="Leyenda") +
theme_minimal()
library(ggplot2)
# Suponiendo que tienes un dataframe 'historia' con las columnas 'epoch', 'accuracy' y 'val_accuracy'
# Por ejemplo: historia <- data.frame(epoch=1:n, accuracy=..., val_accuracy=...)
ggplot(historia_modelo1, aes(x=epoch)) +
geom_line(aes(y=accuracy, colour="Entrenamiento")) +
geom_line(aes(y=val_accuracy, colour="Validación")) +
labs(title='Curva de Aprendizaje del Modelo 1',
x='Épocas',
y='Accuracy',
colour="Leyenda") +
theme_minimal()
library(ggplot2)
# Suponiendo que tienes un dataframe 'historia' con las columnas 'epoch', 'accuracy' y 'val_accuracy'
# Por ejemplo: historia <- data.frame(epoch=1:n, accuracy=..., val_accuracy=...)
ggplot(historia_modelo1.history['accuracy'], aes(x=epoch)) +
geom_line(aes(y=accuracy, colour="Entrenamiento")) +
geom_line(aes(y=val_accuracy, colour="Validación")) +
labs(title='Curva de Aprendizaje del Modelo 1',
x='Épocas',
y='Accuracy',
colour="Leyenda") +
theme_minimal()
View(historia_modelo1)
library(ggplot2)
# Suponiendo que tienes un dataframe 'historia' con las columnas 'epoch', 'accuracy' y 'val_accuracy'
# Por ejemplo: historia <- data.frame(epoch=1:n, accuracy=..., val_accuracy=...)
ggplot(historia_modelo1.metrix['accuracy'], aes(x=epoch)) +
geom_line(aes(y=accuracy, colour="Entrenamiento")) +
geom_line(aes(y=val_accuracy, colour="Validación")) +
labs(title='Curva de Aprendizaje del Modelo 1',
x='Épocas',
y='Accuracy',
colour="Leyenda") +
theme_minimal()
library(ggplot2)
# Suponiendo que tienes un dataframe 'historia' con las columnas 'epoch', 'accuracy' y 'val_accuracy'
# Por ejemplo: historia <- data.frame(epoch=1:n, accuracy=..., val_accuracy=...)
ggplot(historia_modelo1.metrics['accuracy'], aes(x=epoch)) +
geom_line(aes(y=accuracy, colour="Entrenamiento")) +
geom_line(aes(y=val_accuracy, colour="Validación")) +
labs(title='Curva de Aprendizaje del Modelo 1',
x='Épocas',
y='Accuracy',
colour="Leyenda") +
theme_minimal()
library(ggplot2)
# Suponiendo que tienes un dataframe 'historia' con las columnas 'epoch', 'accuracy' y 'val_accuracy'
# Por ejemplo: historia <- data.frame(epoch=1:n, accuracy=..., val_accuracy=...)
ggplot(historia_modelo1$metrics['accuracy'], aes(x=epoch)) +
geom_line(aes(y=accuracy, colour="Entrenamiento")) +
geom_line(aes(y=val_accuracy, colour="Validación")) +
labs(title='Curva de Aprendizaje del Modelo 1',
x='Épocas',
y='Accuracy',
colour="Leyenda") +
theme_minimal()
historia_modelo1$metrics['accuracy']
accuracy <- historia_modelo1$metrics['accuracy']
val_accuracy <- historia_modelo1$metrics['val_accuracy']
# Crear un dataframe a partir de los vectores
historia_df <- data.frame(accuracy = accuracy, val_accuracy = val_accuracy)
View(historia_df)
View(historia_df)
ggplot(historia_df, aes(x=epoch)) +
geom_line(aes(y=accuracy, colour="Entrenamiento")) +
geom_line(aes(y=val_accuracy, colour="Validación")) +
labs(title='Curva de Aprendizaje del Modelo 1',
x='Épocas',
y='Accuracy',
colour="Leyenda") +
theme_minimal()
epochs <- historia_modelo1$params['epochs']
View(epochs)
epochs <- 1:historia_modelo1$params['epochs']
epochs <- 1:50 #historia_modelo1$params['epochs']
# Crear un dataframe a partir de los vectores
historia_df <- data.frame(epoch = epochs, accuracy = accuracy, val_accuracy = val_accuracy)
View(historia_df)
ggplot(historia_df, aes(x=epoch)) +
geom_line(aes(y=accuracy, colour="Entrenamiento")) +
geom_line(aes(y=val_accuracy, colour="Validación")) +
labs(title='Curva de Aprendizaje del Modelo 1',
x='Épocas',
y='Accuracy',
colour="Leyenda") +
theme_minimal()
# Parametrizamos
require(knitr)
require(rmdformats)
## Global options
options(max.print="75")
opts_chunk$set(echo=params$printcode,
cache=TRUE, # para no reproducir cuando no hay cambios
prompt=FALSE,
tidy=TRUE,
comment=NA,
message=FALSE,
warning=FALSE)
opts_knit$set(width=75)
Sys.setlocale("LC_TIME", "C")
# Instalamos paquetes, si no lo están, en caso de ser requeridos
if(!(require(knitr))) install.packages("knitr")
if(!(require(rmdformats))) install.packages("rmdformats")
if(!(require(reticulate))) install.packages("reticulate")
if(!(require(keras))) install.packages("keras")
if(!(require(dplyr))) install.packages("dplyr")
if(!(require(e1071))) install.packages("e1071")
if(!(require(kernlab))) install.packages("kernlab")
if(!(require(caret))) install.packages("caret")
if(!(require(ggplot2))) install.packages("ggplot2")
if(!(require(C50))) install.packages("C50")
if(!(require(randomForest))) install.packages("randomForest")
if(!(require(bibtex))) install.packages("bibtex")
# Fichero human_data.txt con los datos de las secuencias con la clase de proteínas
# Lo hacemos parametrizado
data <- params$data
# Cargamos el archivo ECGCvdata.csv en un DataFrame
ecgcv_rawdata_df <- read.csv(data)
# Obtenemos el número de filas y columnas
dimensiones <- dim(ecgcv_rawdata_df)
num_filas <- dimensiones[1]
num_columnas <- dimensiones[2]
# Cargamos el paquete dplyr Lo utilizaremos para operar con los df
require(dplyr)
# Extraemos las dos primeras columnas de nuestro df y las guardamos.
id_class_columnas <- ecgcv_rawdata_df %>% select (RECORD, ECG_signal)
# Data Frame sin las columnas "RECORD" y "ECG_signal" (class)
ecgcv_rawdata_sin_class_df <- ecgcv_rawdata_df %>% select (-RECORD, -ECG_signal)
# Verificamos que no haya valores anómalos (nulos) en el DataFrame
# Identificamos las columnas que contienen al menos un NaN
columnas_con_NaN <- apply(ecgcv_rawdata_sin_class_df, 2, function(x) any(is.nan(x)))
# Excluimos las columnas con NaN del dataframe
ecgcv_data_sin_NaN_df <- ecgcv_rawdata_sin_class_df[, !columnas_con_NaN]
num_columnas_sin_NaN <- dim(ecgcv_data_sin_NaN_df)[2]
# Añadimos la columna "ECG_signal" (es la "class")
ecgcv_data_df <- cbind(id_class_columnas[,2], ecgcv_data_sin_NaN_df)
# Añadimos nombres a la primera columna "class" (ECG_signal)
names(ecgcv_data_df)[1] <- c("class")
# Convertimos la columna 'class' en factor
ecgcv_data_df$class <- factor(ecgcv_data_df$class)
# Separamos la base de datos en dos grupos: el grupo data.train con el 67 % de
# las observaciones (params$p.train) y el grupo data.test con el resto
# Fijamos la semilla para que los cálculos pseudo-aleatorios sean reproducibles
set.seed(params$seed)
# Obtenemos el número total de filas (datos) de parkinson
n <- nrow(ecgcv_data_df)
# Calculamos el número de filas para entrenamiento y pruebas
n_train <- round(params$p.train * n) # 67%
n_test <- n - n_train # 33%
# Obtenemos los índices de filas al azar para entrenamiento
indices_train <- sample(1:n, n_train)
# Seleccionamos las filas para entrenamiento
data.train <- ecgcv_data_df[indices_train,]
# Seleccionamos las filas restantes para pruebas
data.test <- ecgcv_data_df[-indices_train, ]
require(dplyr)
# Seleccionamos los datos de "train" sin la clase
data.train_sin <- data.train %>% select(-class)
# Creamos los labels de class de "train"
train_labels <- data.train[, 1]
# Seleccionamos los datos de "test" sin la clase
data.test_sin <- data.test %>% select(-class)
# Creamos los labels de class de "test"
test_labels <- data.test[, 1]
# Creamos la función para normalizar con la transformación minmax
normalize_min_max <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
# Aplicar a una columna o a un dataframe completo
data.train_sin.norm <- as.data.frame(lapply(data.train_sin, normalize_min_max))
data.test_sin.norm <- as.data.frame(lapply(data.test_sin, normalize_min_max))
# Convertimos los dataframes a matrix para poder ejecutar los modelos ANN
x_train <- as.matrix(data.train_sin.norm)
x_test <- as.matrix(data.test_sin.norm)
# Restamos 1 ya que las categorías comienzan en 1
y_train <- as.integer(train_labels) - 1
y_test <- as.integer(test_labels) - 1
# Para ejecutar "keras" usamos el entorno virtual en el que està instalado "tensorflow"
require(reticulate)
use_python("/home/sheddad/anaconda3/envs/venv_UOC_ML_conda/bin/python3.10",
required = TRUE)
# Cargamos la librería keras que contiene tensorflow
require(keras)
# Definimos el modelo
mod_ANN_1 <- keras_model_sequential()
# Añadimos una capa oculta densa de 15 nodos con activación relu
input_shape <- c(dim(data.train_sin)[2])
mod_ANN_1 %>%
layer_dense(units = 15, input_shape = c(dim(data.train_sin)[2]), activation = 'relu') %>%
# Añadimos un Dropout del 20%
layer_dropout(rate = 0.2) %>%
# Generamos la capa de salida con activación softmax (4 nodos para las 4 clases)
layer_dense(units = 4, activation = 'softmax')
# Mostramos el resumen del modelo
summary(mod_ANN_1)
# Compilamos el modelo
mod_ANN_1 %>% compile(
loss = 'sparse_categorical_crossentropy',
optimizer = 'adam',
metrics = c('accuracy')
)
# Hacemos que pare en caso de que ya no se mejore más
early_stopping <- callback_early_stopping(
monitor = 'val_loss',
patience = 10,
restore_best_weights = TRUE)
# Ejecutamos el modelo
# Épocas
epochs = 50
historia_mod_ANN_1 <- mod_ANN_1 %>% fit(
x_train, y_train,
epochs = epochs,
batch_size = 32,
validation_split = 0.2,
callbacks=early_stopping)
library(ggplot2)
accuracy <- historia_mod_ANN_1$metrics['accuracy']
val_accuracy <- historia_mod_ANN_1$metrics['val_accuracy']
epochs <- 1:epochs
# Creamos un dataframe a partir de los valores
historia_df <- data.frame(epoch = epochs, accuracy = accuracy,
val_accuracy = val_accuracy)
# Representamos ambas curvas
ggplot(historia_df, aes(x=epoch)) +
geom_line(aes(y=accuracy, colour="Entrenamiento")) +
geom_line(aes(y=val_accuracy, colour="Validación")) +
labs(title='Curva de Aprendizaje del Modelo 1',
x='Épocas',
y='Accuracy',
colour="Leyenda") +
theme_minimal()
# Predicción con "keras" del test con el modelo ANN que produce probabilidades
prob_mod.ANN_1 <- mod_ANN_1 %>% predict(x_test, axis=1)
# Convertimos las probabilidades en clases
pred_mod.ANN_1 <- apply(prob_mod.ANN_1, 1, which.max)
#y las hacemos factores
pred_mod.ANN_1 <- factor(pred_mod.ANN_1, labels = c("AFF", "ARR", "CHF", "NSR"))
# Métricas de rendimiento con "caret"
# Matriz de confusión
res <- table(pred_mod.ANN_1, test_labels)
conf.mod.ANN_1 <- confusionMatrix(res)
# Visualizamos
conf.mod.ANN_1
conf.mod.ANN_1$overall["P-Value"]
# Predicción con "keras" del test con el modelo ANN que produce probabilidades
prob_mod.ANN_1 <- mod_ANN_1 %>% predict(x_test, axis=1)
# Convertimos las probabilidades en clases
pred_mod.ANN_1 <- apply(prob_mod.ANN_1, 1, which.max)
#y las hacemos factores
pred_mod.ANN_1 <- factor(pred_mod.ANN_1, labels = c("AFF", "ARR", "CHF", "NSR"))
# Métricas de rendimiento con "caret"
# Matriz de confusión
res <- table(pred_mod.ANN_1, test_labels)
conf.mod.ANN_1 <- confusionMatrix(res)
# Visualizamos
conf.mod.ANN_1
conf.mod.ANN_1$overall["P-Value"]
View(conf.mod.ANN_1)
